# yaml-language-server: $schema=../../../schema/local.json
config:
  tracing:
    otlpEndpoint: http://localhost:4317
    randomSampling: true
    fields:
      add:
        span.name: '"openai.chat"'
        gen_ai.operation.name: '"chat"'
        gen_ai.system: 'llm.provider'
        gen_ai.prompt: 'flatten_recursive(llm.prompt)'
        gen_ai.completion: 'flatten_recursive(llm.completion.map(c, {"role":"assistant", "content": c}))'
        gen_ai.usage.completion_tokens: 'llm.output_tokens'
        gen_ai.usage.prompt_tokens: 'llm.input_tokens'
        gen_ai.request.model: 'llm.request_model'
        gen_ai.response.model: 'llm.response_model'
        gen_ai.request: 'flatten(llm.params)'
        llm.is_streaming: 'llm.streaming'
binds:
- port: 3000
  listeners:
  - routes:
    - backends:
       - ai:
          name: gemini
          provider:
              gemini:
                # Optional; overrides the model in requests
                model: gemini-1.5-flash
      policies:
        backendAuth:
          key: $GEMINI_API_KEY
