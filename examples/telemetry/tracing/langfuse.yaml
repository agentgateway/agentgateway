# yaml-language-server: $schema=../../../schema/local.json
config:
  tracing:
    otlpEndpoint: http://localhost:4317
    randomSampling: true
    fields:
      add:
        gen_ai.operation.name: '"chat"'
        gen_ai.system: "llm.provider"
        gen_ai.prompt: "llm.prompt"
        gen_ai.completion: 'llm.completion.map(c, {"role":"assistant", "content": c})'
        gen_ai.usage.completion_tokens: "llm.output_tokens"
        gen_ai.usage.prompt_tokens: "llm.input_tokens"
        # Langfuse uses the wrong one here! Intentionally swap
        gen_ai.request.model: "llm.request_model"
        gen_ai.response.model: "llm.response_model"
        gen_ai.request: "flatten(llm.params)"
binds:
  - port: 3000
    listeners:
      - routes:
          - backends:
              - ai:
                  name: gemini
                  provider:
                    gemini:
                      # Optional; overrides the model in requests
                      model: gemini-1.5-flash
            policies:
              backendAuth:
                key: $GEMINI_API_KEY
